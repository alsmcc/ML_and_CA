# The impact of Computer Architecture on Machine Learning

Here I demonstrate the effect of taking advantage of the Computer Architecture on the Gradient Descent Algorithm.

For Optimization the Gradient Descent Algorithm is with out any doubt the best starting point to get an understanding for optimization algorithms. Ther are many derivatives of this algorithm get to the global minimum, or to at least near to it, faster. But the basic principle stays the same. Here I implement in a basic way that makes it easer to understand:

1. *Batch* Gradient Descent
2. *Stochastic* Gradient Descent
3. Gradient Descent with *Mini-Batches*
4. Gradient Descent with *Mini-Batches* and *GPU-Parallization*
